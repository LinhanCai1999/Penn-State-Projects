{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "## Penn State DS 200 Fall 2019\n## Lab 4A Tweets Gathering\nIn this lab, you will learn to gather tweets using keywords and hashtags you \nobtained from your Twitter Developer account.\n\n### Install Tweepy\nThe first thing we will do is to install a tweepy, a Python library/module for gathering tweets using Twitter API."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install tweepy\n!pip install datascience",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting tweepy\n  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\nCollecting PySocks>=1.5.7 (from tweepy)\n  Downloading https://files.pythonhosted.org/packages/8d/59/b4572118e098ac8e46e399a1dd0f2d85403ce8bbaad9ec79373ed6badaf9/PySocks-1.7.1-py3-none-any.whl\nRequirement already satisfied: six>=1.10.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from tweepy) (1.11.0)\nRequirement already satisfied: requests>=2.11.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from tweepy) (2.14.2)\nCollecting requests-oauthlib>=0.7.0 (from tweepy)\n  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->tweepy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n\u001b[K     |████████████████████████████████| 153kB 1.9MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: PySocks, oauthlib, requests-oauthlib, tweepy\nSuccessfully installed PySocks-1.7.1 oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.8.0\n\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nCollecting datascience\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/c1/fff066029adeaaafe12d59a906440da8ce2dcffc4cf852e9474b80a04dad/datascience-0.15.3.tar.gz (42kB)\n\u001b[K     |████████████████████████████████| 51kB 2.2MB/s eta 0:00:01\n\u001b[?25hCollecting folium>=0.9.1 (from datascience)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/ff/004bfe344150a064e558cb2aedeaa02ecbf75e60e148a55a9198f0c41765/folium-0.10.0-py2.py3-none-any.whl (91kB)\n\u001b[K     |████████████████████████████████| 92kB 3.9MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: sphinx in /home/nbuser/anaconda3_420/lib/python3.5/site-packages/Sphinx-1.4.6-py3.5.egg (from datascience) (1.4.6)\nRequirement already satisfied: setuptools in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (41.1.0)\nCollecting matplotlib>=3.0.0 (from datascience)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/61/465fb3bfba684b0f53b5c4829c3c89e86e6fe9fdcdfda93e38f1788090f0/matplotlib-3.0.3-cp35-cp35m-manylinux1_x86_64.whl (13.0MB)\n\u001b[K     |████████████████████████████████| 13.0MB 12kB/s  eta 0:00:01    |██████                          | 2.4MB 2.5MB/s eta 0:00:05     |████████████████▋               | 6.7MB 3.0MB/s eta 0:00:03\n\u001b[?25hRequirement already satisfied: pandas in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (0.19.2)\nRequirement already satisfied: scipy in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (1.1.0)\nRequirement already satisfied: numpy in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (1.17.0)\nRequirement already satisfied: ipython in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (6.2.1)\nRequirement already satisfied: pytest in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (2.9.2)\nCollecting coverage==4.5.3 (from datascience)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/d1/df283ea8e30aff9d567b111f40184260caa4a54f55aa26b48b82cf79161e/coverage-4.5.3-cp35-cp35m-manylinux1_x86_64.whl (205kB)\n\u001b[K     |████████████████████████████████| 215kB 4.5MB/s eta 0:00:01\n\u001b[?25hCollecting coveralls (from datascience)\n  Downloading https://files.pythonhosted.org/packages/b5/6a/bd33b20b03eb8d596e6b7ccfea66ca3b85fadf55ae8e6086091f498fc3d6/coveralls-1.8.2-py2.py3-none-any.whl\nRequirement already satisfied: bokeh in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (0.12.7)\nCollecting branca>=0.3.0 (from folium>=0.9.1->datascience)\n  Downloading https://files.pythonhosted.org/packages/63/36/1c93318e9653f4e414a2e0c3b98fc898b4970e939afeedeee6075dd3b703/branca-0.3.1-py3-none-any.whl\nRequirement already satisfied: requests in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from folium>=0.9.1->datascience) (2.14.2)\nCollecting jinja2>=2.9 (from folium>=0.9.1->datascience)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/e0/eb35e762802015cab1ccee04e8a277b03f1d8e53da3ec3106882ec42558b/Jinja2-2.10.3-py2.py3-none-any.whl (125kB)\n\u001b[K     |████████████████████████████████| 133kB 9.4MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six>=1.5 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (1.11.0)\nRequirement already satisfied: Pygments>=2.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (2.1.3)\nRequirement already satisfied: docutils>=0.11 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (0.12)\nRequirement already satisfied: snowballstemmer>=1.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (1.2.1)\nRequirement already satisfied: babel!=2.0,>=1.3 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (2.3.4)\nRequirement already satisfied: alabaster<0.8,>=0.7 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (0.7.9)\nRequirement already satisfied: imagesize in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (0.7.1)\nCollecting kiwisolver>=1.0.1 (from matplotlib>=3.0.0->datascience)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/18/4cd2e84c6aff0c6a50479118083d20b9e676e5175a913c0ea76d700fc244/kiwisolver-1.1.0-cp35-cp35m-manylinux1_x86_64.whl (90kB)\n\u001b[K     |████████████████████████████████| 92kB 3.8MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from matplotlib>=3.0.0->datascience) (2.8.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from matplotlib>=3.0.0->datascience) (2.1.4)\nRequirement already satisfied: cycler>=0.10 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from matplotlib>=3.0.0->datascience) (0.10.0)\nRequirement already satisfied: pytz>=2011k in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from pandas->datascience) (2016.6.1)\nRequirement already satisfied: jedi>=0.10 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (0.11.0)\nRequirement already satisfied: decorator in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (4.4.0)\nRequirement already satisfied: pickleshare in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (0.7.4)\nRequirement already satisfied: simplegeneric>0.8 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (0.8.1)\nRequirement already satisfied: traitlets>=4.2 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (4.3.1)\nRequirement already satisfied: prompt_toolkit<2.0.0,>=1.0.4 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (1.0.15)\nRequirement already satisfied: pexpect in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (4.0.1)\nRequirement already satisfied: py>=1.4.29 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from pytest->datascience) (1.4.31)\nCollecting docopt>=0.6.1 (from coveralls->datascience)\n  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\nRequirement already satisfied: PyYAML>=3.10 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from bokeh->datascience) (3.13)\nRequirement already satisfied: tornado>=4.3 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from bokeh->datascience) (4.4.1)\nRequirement already satisfied: bkcharts>=0.2 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from bokeh->datascience) (0.2)\nRequirement already satisfied: MarkupSafe>=0.23 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from jinja2>=2.9->folium>=0.9.1->datascience) (0.23)\nRequirement already satisfied: parso==0.1.* in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from jedi>=0.10->ipython->datascience) (0.1.1)\nRequirement already satisfied: wcwidth in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from prompt_toolkit<2.0.0,>=1.0.4->ipython->datascience) (0.1.7)\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Building wheels for collected packages: datascience, docopt\n  Building wheel for datascience (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for datascience: filename=datascience-0.15.3-cp35-none-any.whl size=44582 sha256=9fbc7638de8baac43a1f536b24f4a4c6d57fbc8e72499a8d5d7a473510b54ac1\n  Stored in directory: /home/nbuser/.cache/pip/wheels/b8/37/0a/80274866028f6485c5957f0e1acf8e2b755fbe9dd0fd4ad275\n  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=19851 sha256=f6d3863ff68572585c9e3e48503a90db8c4f149591d7dc192de7eef70cbf4115\n  Stored in directory: /home/nbuser/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\nSuccessfully built datascience docopt\nInstalling collected packages: jinja2, branca, folium, kiwisolver, matplotlib, coverage, docopt, coveralls, datascience\n  Found existing installation: Jinja2 2.8\n    Uninstalling Jinja2-2.8:\n      Successfully uninstalled Jinja2-2.8\n  Found existing installation: matplotlib 2.1.1\n    Uninstalling matplotlib-2.1.1:\n      Successfully uninstalled matplotlib-2.1.1\nSuccessfully installed branca-0.3.1 coverage-4.5.3 coveralls-1.8.2 datascience-0.15.3 docopt-0.6.2 folium-0.10.0 jinja2-2.10.3 kiwisolver-1.1.0 matplotlib-3.0.3\n\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tweepy\nfrom tweepy import OAuthHandler\nfrom tweepy import Stream\nfrom tweepy.streaming import StreamListener\n\n\n\nimport sys\nimport os\nimport json\nimport time\nimport datetime\nimport re\n\nimport pandas as pd",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Python Code for Gathering Tweets\nThe following code defines a group of code that, together, \"listens\" (responds) to tweets (sent from Twitter API) that match the keywords and hashtags specified.  The code also filters out non-English tweets, and performs some simple preprocessing (e.g., remove non-ASCII characters in the body of the tweet), so that we do not need to worry about them later."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class MyListener(StreamListener):\n    def __init__(self, raw_file, csv_file, text_file, max_num=300):\n        super().__init__()\n        self.raw_file = raw_file\n        self.csv_file = csv_file\n        self.text_file = text_file\n        self.max_num = max_num\n        self.count = 0\n        self.start_time = time.time()\n\n    def on_data(self, data):\n        # Filter out special cases\n        if data.startswith('{\"limit\":'):\n            return\n\n        # Filter out non-English tweets\n        tweet = json.loads(data)\n        if tweet['lang'] != 'en':\n            return\n        # if 'retweeted_status' in tweet:\n        #     return\n\n        # Extract fields from tweet and write to csv_file\n        user_id = tweet['user']['id']\n        user_name = tweet['user']['name']\n        tweet_time = tweet['created_at']\n        location = tweet['user']['location']\n        text = tweet['text'].strip().replace('\\n', ' ').replace('\\t', ' ')\n\n        # Remove non-ASCII characters and commas in user_name and location\n        if user_name is not None:\n            user_name = ''.join([c if ord(c) < 128 else '' for c in user_name])\n            user_name = user_name.replace(',', '')\n        if location is not None:\n            location = ''.join([c if ord(c) < 128 else '' for c in location])\n            location = location.replace(',', '')\n\n        # Remove non-ASCII characters in text\n        text = ''.join([c if ord(c) < 128 else '' for c in text])\n        # Replace commas with space\n        text = text.replace(',', ' ')\n        # Replace double quotes with blanks\n        text = re.sub(r'\\\"', '', text)\n        # Replace consecutive underscores with space\n        text = re.sub(r'[_]{2,}', ' ', text)\n        # Remove all consecutive whitespace characters\n        text = ' '.join(text.split())\n\n        # Check if csv_file, text_file exist\n        # If not, create them and write the heads\n        if not os.path.isfile(self.csv_file):\n            with open(self.csv_file, 'w') as f:\n                f.write(','.join(['user_id', 'user_name', 'tweet_time', 'location', 'text']) + '\\n')\n        if not os.path.isfile(self.text_file):\n            with open(self.text_file, 'w') as f:\n                f.write('text\\n')\n\n        with open(self.raw_file, 'a') as f_raw, open(self.csv_file, 'a') as f_csv, open(self.text_file, 'a') as f_text:\n            # Write to files\n            f_raw.write(data.strip() + '\\n')\n            f_csv.write(','.join(map(str, [user_id, user_name, tweet_time, location, text])) + '\\n')\n            f_text.write(text + '\\n')\n\n            # Increment count\n            self.count += 1\n            # if self.count % 10 == 0 and self.count > 0:\n            sys.stdout.write('\\r{}/{} tweets downloaded'.format(self.count, self.max_num))\n            sys.stdout.flush()\n\n            # Check if reaches the maximum tweets number limit\n            if self.count == self.max_num:\n                print('\\nMaximum number reached.')\n                end_time = time.time()\n                elapse = end_time - self.start_time\n                print('It took {} seconds to download {} tweets'.format(elapse, self.max_num))\n                sys.exit(0)\n\n    def on_error(self, status):\n        print(status)\n        return True\n\n# Get the str representation of the current date and time    \ndef current_datetime_str():\n    return format(datetime.datetime.now(), \"%Y-%m-%d_%H-%M-%S\")",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Exercise 4.1 Paste your API Keys and Access Tokens into the Tweet Gathering Code\n#### Note: Make sure you copy each code exactly as they are.  Especially, pay attention to the first character and the last character to make sure you did not miss any of them.  Also, double check you did not accidentently include space or left parenthesis when you copy keys and token.\n#### Create a keywords.txt file directly in Jupyter Notebook or upload it from your computer."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def main():\n    # Paste your keys and token below.  \n    consumer_key = 'jpaaqDJwGcnlVHo7MwroL3rGp'\n    consumer_secret = '5LRMPGExsWP3CWAOFrHYjxl1VYt3k4cTYVsYiIUARi9L5eFbT7'\n    access_token = '765609343014219776-Zn5OIY664vWdgAlVBs1KNSZwxMKeUXP'\n    access_secret = '3aGmyvkWwfOGIbqoD3WMbwbprAlmGwxkMShb5F0oFeyw9'\n\n    auth = OAuthHandler(consumer_key, consumer_secret)\n    auth.set_access_token(access_token, access_secret)\n    api = tweepy.API(auth)\n\n    # Welcome\n    print('===========================================================')\n    print('Welcome to the user interface of gathering tweets pipeline!')\n    print('You can press \"Ctrl+C\" at anytime to abort the program.')\n    print('===========================================================')\n    print()\n\n    # Prompt for input keywords\n    methods = ['manual', 'file']\n    print('How do you want to specify your key words?')\n    while True:\n        m = input('Type \"manual\" or \"file\" >>> ')\n        if m in methods:\n            break\n        else:\n            print('\\\"{}\\\" is an invalid input! Please try again.\\n'.format(m))\n\n    # Choose keywords:\n    if m == 'file':\n        print('===========================================================')\n        print('Please input the file name that contains your key words.')\n        print('Notes:')\n        print('    The file should contain key words in one or multiple lines, and multiple key words should be separated by *COMMA*.')\n        print('        For example: NBA, basketball, Lebron James')\n        print('    If the file is under the current directory, you can directly type the file name, e.g., \"keywords.txt\".')\n        print('    If the file is in another directory, please type the full file name, e.g., \"C:\\\\Downloads\\\\keywords.txt\" (for Windows), or \"/Users/xy/Downloads/keywords.txt\" (for MacOS/Linux).')\n\n        while True:\n            file_name = input('Type your file name >>> ')\n            if os.path.isfile(file_name):\n                break\n            else:\n                print('\"{}\" is not a valid file name! Please check if the file exists.\\n'.format(file_name))\n\n        # Check the content of keywords file\n        key_words = []\n        with open(file_name, 'r') as f:\n            lines = f.readlines()\n            if len(lines) == 0:\n                print('\\n{} is an empty file!\\nTask aborted!'.format(file_name))\n                sys.exit(1)\n\n            for line in lines:\n                line = line.strip()\n                # Detect non-ASCII characters\n                for c in line:\n                    if ord(c) >= 128:\n                        print('\\n{} contains non-ASCII characters: \"{}\" \\nPlease remove them and try again'.format(file_name, c))\n                        sys.exit(1)\n                # Check delimiters\n                if line.count(' ') > 1 and ',' not in line:\n                    print('\\nMore than 1 <space> symbols exist in the key words file, but none comma exists')\n                    print('I\\'m confused about your keywords. Please separate your key words by commas.')\n                    sys.exit(1)\n\n                words = line.split(',')\n                for w in words:\n                    if len(w.strip()) > 0:\n                        key_words.append(w.strip())\n\n        # Check key_words\n        if len(key_words) == 0:\n            print('\\nZero key words are found in {}! Please check your key words file.'.format(file_name))\n            sys.exit(1)\n\n    elif m == 'manual':\n        print('===========================================================')\n        print('Please input your key words (separated by comma), and hit <ENTER> when done.')\n\n        while True:\n            line = input('Type the key words >>> ')\n            line = line.strip()\n\n            invalid_flag = False\n            # Check empty\n            if len(line) == 0:\n                print('\\nYour input is empty! Please try again.')\n                invalid_flag = True\n            # Detect non-ASCII characters\n            for c in line:\n                if ord(c) >= 128:\n                    print('\\nYour input contains non-ASCII characters: \"{}\"! Please try again.'.format(c))\n                    invalid_flag = True\n                    break\n            # Check delimiters\n            if line.count(' ') > 1 and ',' not in line:\n                print('\\nMore than 1 <space> symbols exist in your input, but none comma exists')\n                print('I\\'m confused about your keywords. Please try again')\n                invalid_flag = True\n\n            if invalid_flag:\n                continue\n            else:\n                break\n\n        # Process input\n        key_words = []\n        for w in line.split(','):\n            if len(w.strip()) > 0:\n                key_words.append(w.strip())\n\n    # Print valid key words\n    key_words = list(set(key_words))\n    print('\\n{} unique key words being used: '.format(len(key_words)), key_words)\n\n    # Prompt for number of tweets to be gathered\n    print('===========================================================')\n    print('How many tweets do you want to gather? \\nInput an integer number, or just hit <ENTER> to use the default number 300.')\n    num_tweets = 300\n    while True:\n        s = input('Input an integer >>> ')\n        s = s.strip()\n        if len(s) == 0:\n            break\n        elif s.isdigit():\n            num = int(s)\n            if num > 0:\n                num_tweets = num\n                break\n            else:\n                print('\\nPlease input a number that is greater than 0.')\n        else:\n            print('\\nPlease input a valid integer number.')\n\n    print('{} tweets to be gathered.'.format(num_tweets))\n\n    # Streaming\n    # TODO: remvoe '\\t', '\\n' and ',' in text field, also remove empty text\n    print('===========================================================')\n    print('Start gathering tweets ...')\n\n    postfix = current_datetime_str()\n    raw_file = 'raw_{}.json'.format(postfix)\n    csv_file = 'data_{}.csv'.format(postfix)\n    text_file = 'text_{}.csv'.format(postfix)\n\n    twitter_stream = Stream(auth, MyListener(raw_file=raw_file, csv_file=csv_file, text_file=text_file, max_num=num_tweets))\n    twitter_stream.filter(track=key_words)\n\n\nif __name__ == '__main__':\n    try:\n        main()\n    except KeyboardInterrupt:\n        print('\\nTask aborted!')\n        \n",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "===========================================================\nWelcome to the user interface of gathering tweets pipeline!\nYou can press \"Ctrl+C\" at anytime to abort the program.\n===========================================================\n\nHow do you want to specify your key words?\nType \"manual\" or \"file\" >>> file\n===========================================================\nPlease input the file name that contains your key words.\nNotes:\n    The file should contain key words in one or multiple lines, and multiple key words should be separated by *COMMA*.\n        For example: NBA, basketball, Lebron James\n    If the file is under the current directory, you can directly type the file name, e.g., \"keywords.txt\".\n    If the file is in another directory, please type the full file name, e.g., \"C:\\Downloads\\keywords.txt\" (for Windows), or \"/Users/xy/Downloads/keywords.txt\" (for MacOS/Linux).\nType your file name >>> keywords.txt\n\n3 unique key words being used:  ['#tradewar', 'tradewar', 'Trade War']\n===========================================================\nHow many tweets do you want to gather? \nInput an integer number, or just hit <ENTER> to use the default number 300.\nInput an integer >>> \n300 tweets to be gathered.\n===========================================================\nStart gathering tweets ...\n300/300 tweets downloaded\nMaximum number reached.\nIt took 2046.3333098888397 seconds to download 300 tweets\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}